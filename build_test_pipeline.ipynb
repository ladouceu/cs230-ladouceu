{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d6179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6b2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51803586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d72f831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# must be >=2.6.2\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef16baae",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\"frame\":0, \"id\":1, \"x\":2, \"y\":3, \"w\":4, \"h\":5, \"x1\":6, \"x2\":7, \"conf\":9}\n",
    "threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f51dde4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on file: data/test/MOT16-01/det/det.txt\n",
      "[[ 1.0000e+00  1.0000e+00  7.7268e+02  4.5543e+02  4.1871e+01  1.2761e+02\n",
      "   2.1262e+00 -1.0000e+00 -1.0000e+00 -1.0000e+00]\n",
      " [ 1.0000e+00  2.0000e+00  7.1779e+02  4.5129e+02  4.4948e+01  1.3684e+02\n",
      "   1.7969e+00 -1.0000e+00 -1.0000e+00 -1.0000e+00]\n",
      " [ 1.0000e+00  3.0000e+00  2.3074e+02  4.6507e+02  2.1974e+01  6.7922e+01\n",
      "   1.6718e+00 -1.0000e+00 -1.0000e+00 -1.0000e+00]\n",
      " [ 1.0000e+00  4.0000e+00  1.0017e+03  4.5586e+02  6.3980e+01  1.9394e+02\n",
      "   6.3705e-01 -1.0000e+00 -1.0000e+00 -1.0000e+00]\n",
      " [ 1.0000e+00  5.0000e+00  7.0264e+02  3.7421e+02  7.3643e+01  2.2293e+02\n",
      "  -1.0042e-01 -1.0000e+00 -1.0000e+00 -1.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "videos = glob.glob(\"data/test/*/det/det.txt\")\n",
    "\n",
    "for video in videos:\n",
    "    print(\"Evaluating on file: {}\".format(file))\n",
    "    data_video = np.loadtxt(file, delimiter=\",\")\n",
    "    frames = np.unique(data[:,header[\"frame\"]])\n",
    "    # put an id on all objects in the first frame\n",
    "    first_frame = min(frames)\n",
    "    data_first_frame = data_video[data_video[:,header[\"frame\"]]==first_frame,:]\n",
    "    largest_id = len(data_old_frame)\n",
    "    old_objs = # all objs from first frame in a tensor\n",
    "    old_id_map = list(range(1,largest_id+1)) # map from position in tensor above to id of object\n",
    "    data_first_frame[:,header[\"id\"]] = old_id_map\n",
    "    \n",
    "    # iterate through frames, on each next frame find the best match from\n",
    "    # the objects of the previous match and remember the matching score. \n",
    "    # The matching score must be larger than a certain threshold. Not \n",
    "    # more than 1 objects in the next frame can be matched to one object in the\n",
    "    # previous frame, so recursively go through all unmatched objects until they\n",
    "    # are all matched. If no match is found for an object, assigned a new id \n",
    "    # being largest_id + 1 \n",
    "    for frame in frames:\n",
    "        data_new_frame = data_video[data_video[:,header[\"frame\"]]==frame,:]\n",
    "        new_objs = # all objs from this new frame in a tensor\n",
    "        new_id_map = [] # map from position in tensor above to id of object\n",
    "        for r,det in enumerate(data_new_frame):\n",
    "            new_obj = # a tensor of all same pictures with same dim as old_obj\n",
    "            similarities = compute_similarity(new_obj, old_obj)\n",
    "            i = np.argmax(similarities)\n",
    "            score = similarities[i]\n",
    "            if score > thresh:\n",
    "                new_id_map += [old_id_map[i]]\n",
    "            else:\n",
    "                largest_id += 1\n",
    "                new_id_map += [largest_id]\n",
    "        \n",
    "        data_new_frame[:,header[\"id\"]] = new_id_map\n",
    "        \n",
    "        #update old variables\n",
    "        old_objs = new_objs\n",
    "        old_id_map = new_id_map\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840669df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0000e+00, -1.0000e+00,  7.7268e+02, ..., -1.0000e+00,\n",
       "        -1.0000e+00, -1.0000e+00],\n",
       "       [ 1.0000e+00, -1.0000e+00,  7.1779e+02, ..., -1.0000e+00,\n",
       "        -1.0000e+00, -1.0000e+00],\n",
       "       [ 1.0000e+00, -1.0000e+00,  2.3074e+02, ..., -1.0000e+00,\n",
       "        -1.0000e+00, -1.0000e+00],\n",
       "       ...,\n",
       "       [ 4.5000e+02, -1.0000e+00,  1.7248e+02, ..., -1.0000e+00,\n",
       "        -1.0000e+00, -1.0000e+00],\n",
       "       [ 4.5000e+02, -1.0000e+00,  2.8900e+02, ..., -1.0000e+00,\n",
       "        -1.0000e+00, -1.0000e+00],\n",
       "       [ 4.5000e+02, -1.0000e+00,  1.6055e+03, ..., -1.0000e+00,\n",
       "        -1.0000e+00, -1.0000e+00]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(files[0],delimiter=\",\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "843b666a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a = data[0:1,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de72ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fcc36459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fb47596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
       "        12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,  22.,\n",
       "        23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,  33.,\n",
       "        34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,  44.,\n",
       "        45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,  55.,\n",
       "        56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,  66.,\n",
       "        67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,  77.,\n",
       "        78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,  88.,\n",
       "        89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,  99.,\n",
       "       100., 101., 102., 103., 104., 105., 106., 107., 108., 109., 110.,\n",
       "       111., 112., 113., 114., 115., 116., 117., 118., 119., 120., 121.,\n",
       "       122., 123., 124., 125., 126., 127., 128., 129., 130., 131., 132.,\n",
       "       133., 134., 135., 136., 137., 138., 139., 140., 141., 142., 143.,\n",
       "       144., 145., 146., 147., 148., 149., 150., 151., 152., 153., 154.,\n",
       "       155., 156., 157., 158., 159., 160., 161., 162., 163., 164., 165.,\n",
       "       166., 167., 168., 169., 170., 171., 172., 173., 174., 175., 176.,\n",
       "       177., 178., 179., 180., 181., 182., 183., 184., 185., 186., 187.,\n",
       "       188., 189., 190., 191., 192., 193., 194., 195., 196., 197., 198.,\n",
       "       199., 200., 201., 202., 203., 204., 205., 206., 207., 208., 209.,\n",
       "       210., 211., 212., 213., 214., 215., 216., 217., 218., 219., 220.,\n",
       "       221., 222., 223., 224., 225., 226., 227., 228., 229., 230., 231.,\n",
       "       232., 233., 234., 235., 236., 237., 238., 239., 240., 241., 242.,\n",
       "       243., 244., 245., 246., 247., 248., 249., 250., 251., 252., 253.,\n",
       "       254., 255., 256., 257., 258., 259., 260., 261., 262., 263., 264.,\n",
       "       265., 266., 267., 268., 269., 270., 271., 272., 273., 274., 275.,\n",
       "       276., 277., 278., 279., 280., 281., 282., 283., 284., 285., 286.,\n",
       "       287., 288., 289., 290., 291., 292., 293., 294., 295., 296., 297.,\n",
       "       298., 299., 300., 301., 302., 303., 304., 305., 306., 307., 308.,\n",
       "       309., 310., 311., 312., 313., 314., 315., 316., 317., 318., 319.,\n",
       "       320., 321., 322., 323., 324., 325., 326., 327., 328., 329., 330.,\n",
       "       331., 332., 333., 334., 335., 336., 337., 338., 339., 340., 341.,\n",
       "       342., 343., 344., 345., 346., 347., 348., 349., 350., 351., 352.,\n",
       "       353., 354., 355., 356., 357., 358., 359., 360., 361., 362., 363.,\n",
       "       364., 365., 366., 367., 368., 369., 370., 371., 372., 373., 374.,\n",
       "       375., 376., 377., 378., 379., 380., 381., 382., 383., 384., 385.,\n",
       "       386., 387., 388., 389., 390., 391., 392., 393., 394., 395., 396.,\n",
       "       397., 398., 399., 400., 401., 402., 403., 404., 405., 406., 407.,\n",
       "       408., 409., 410., 411., 412., 413., 414., 415., 416., 417., 418.,\n",
       "       419., 420., 421., 422., 423., 424., 425., 426., 427., 428., 429.,\n",
       "       430., 431., 432., 433., 434., 435., 436., 437., 438., 439., 440.,\n",
       "       441., 442., 443., 444., 445., 446., 447., 448., 449., 450.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = np.unique(data[:,header[\"frame\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b998170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "486bb848",
   "metadata": {},
   "source": [
    "# Testing my model imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ffb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from siamese_model_image_location import get_siamese_model\n",
    "from train_val_gen_image_location import triplet_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e71aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (200,200)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bd6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the train dataset\n",
    "train_dataset = tf.data.Dataset.from_generator(triplet_gen(\"train\", batch_size, target_shape), ((tf.float32,tf.float32),(tf.float32,tf.float32),(tf.float32,tf.float32)), ((target_shape + (3,),(4,)),(target_shape + (3,),(4,)),(target_shape + (3,),(4,))))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=2^14)\n",
    "train_dataset = train_dataset.batch(batch_size, drop_remainder=True) #TODO: should set to False?\n",
    "train_dataset = train_dataset.prefetch(8)\n",
    "\n",
    "# build the test dataset\n",
    "val_dataset = tf.data.Dataset.from_generator(triplet_gen(\"val\", batch_size, target_shape), ((tf.float32,tf.float32),(tf.float32,tf.float32),(tf.float32,tf.float32)), ((target_shape + (3,),(4,)),(target_shape + (3,),(4,)),(target_shape + (3,),(4,))))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=2^14)\n",
    "val_dataset = val_dataset.batch(batch_size, drop_remainder=True) #TODO: should set to False?\n",
    "val_dataset = val_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7318e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build (and load if load_model_path is not None) the siamese_model\n",
    "siamese_model = get_siamese_model(target_shape, load_model_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "    166/Unknown - 1503s 9s/step - loss: 0.2040"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "siamese_model.fit(train_dataset, epochs=10, validation_data=val_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc9802c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "INFO:tensorflow:Assets written to: models/siamese_network_2/model_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  category=CustomMaskWarning)\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "siamese_model.siamese_network.save(\"models/siamese_network_2/model_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c6f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "siamese_model = get_siamese_model(target_shape, load_model_path = \"models/siamese_network_2/model_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86d313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([0.0027987 , 0.43512282, 0.11399516, 0.21958008, 0.6719003 ,\n",
      "       0.0356839 , 0.32577905, 0.11854939, 0.05120662, 0.06752269,\n",
      "       0.5898831 , 0.72008634, 0.31534147, 0.39584357, 0.9709426 ,\n",
      "       0.2655549 , 0.7045237 , 0.28984952, 1.5087411 , 0.16760036,\n",
      "       1.4004412 , 0.12871963, 0.70179796, 0.7404676 , 0.19323258,\n",
      "       0.38675585, 0.64490104, 0.48554426, 0.58698106, 0.91979396,\n",
      "       0.9395615 , 0.03203043, 0.06709439, 0.27407357, 0.07012771,\n",
      "       0.1825794 , 1.288773  , 0.06101909, 0.06845597, 0.31462198,\n",
      "       0.9549992 , 0.20126264, 0.11504369, 0.72868454, 0.25455585,\n",
      "       0.25545657, 0.08357295, 0.16108814, 0.60716534, 0.24333848,\n",
      "       1.2144446 , 1.0054178 , 0.11314649, 0.93689275, 1.071589  ,\n",
      "       0.00949483, 0.79136837, 0.70707566, 0.8960537 , 0.45198822,\n",
      "       1.2047791 , 1.4274633 , 0.27333677, 1.8147753 ], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([2.556416  , 2.4612484 , 0.48045343, 4.154104  , 0.9331322 ,\n",
      "       0.36321235, 4.059251  , 0.96398556, 2.9130359 , 4.1356544 ,\n",
      "       1.3148893 , 0.73506033, 2.51254   , 1.7580147 , 3.9824896 ,\n",
      "       4.6639476 , 2.9709747 , 0.3634964 , 4.514056  , 0.8198713 ,\n",
      "       1.8745966 , 1.6914986 , 0.8561841 , 2.5427923 , 1.9225795 ,\n",
      "       6.2711325 , 4.043633  , 4.450987  , 0.6748388 , 0.5660069 ,\n",
      "       1.2918835 , 4.0866194 , 3.3101013 , 0.72274363, 1.0258375 ,\n",
      "       4.11123   , 4.059251  , 0.45176098, 5.444255  , 2.1109025 ,\n",
      "       1.0486665 , 0.8951964 , 2.5992408 , 1.0806355 , 0.6893552 ,\n",
      "       1.3990048 , 3.1618032 , 0.9190929 , 2.9458506 , 0.98585033,\n",
      "       4.838446  , 2.9177833 , 2.6885836 , 5.897094  , 2.1385975 ,\n",
      "       4.882596  , 1.5263474 , 4.7078104 , 1.0521142 , 4.176474  ,\n",
      "       3.0220585 , 1.164666  , 1.6863021 , 5.5756407 ], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "for xi in train_dataset:\n",
    "    print(siamese_model(xi))\n",
    "    break\n",
    "# get the embedding, we really want to save the embedding rather than the whole model.. and then run the test examples on the embedding (\n",
    "# the test examples are not triplets but rather just an image and a location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72897772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
